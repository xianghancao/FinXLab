{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db500a6c-eeea-4575-b144-eca217d2d248",
   "metadata": {},
   "source": [
    "# 预训练模型的微调\n",
    "Hans Cao, First: Dec 6, 2024; Update: Dec 6, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336c4c5-7321-4cf6-b3dc-aa9fec377271",
   "metadata": {},
   "source": [
    "**预训练模型的微调（Fine-tuning）** 是指将一个在大规模数据集（例如Wikipedia、新闻文章或特定领域数据）上经过预训练的通用模型（如BERT、GPT或FinBERT）调整为适应特定下游任务的过程。通过微调，可以优化模型参数，使其在目标应用中的表现更优，同时利用其在预训练阶段所获得的知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03113ccb-d348-427d-bfdb-e51c5114b692",
   "metadata": {},
   "source": [
    "### Step 1: Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cee2c-6091-4f23-8abf-a6959ab8eb30",
   "metadata": {},
   "source": [
    "金融短语库Financial Phrase Bank）是一个用于金融情感分类的公共数据集，该数据集来自金融新闻。该数据集包含4840条来自英语金融新闻的句子，并根据情感进行分类。数据集按5至8名注释员的同意率进行划分。数据集地址： https://huggingface.co/datasets/takala/financial_phrasebank 这里选择了 \"sentences_allagree\" 配置，表示使用标注一致性最高的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e63f8ae0-e16d-4121-aaf4-8d69395176e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3d05571c2f49619ad683519c1aa1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:07<00:00, 2.91MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:06<00:00, 3.20MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:09<00:00, 4.42MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ad612a7cc1447fb21aa02c096d27b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fad6a3b55bd443d921d755755163069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e3a79d7c02499e801bc65da34e3ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# 选择 financial_phrasebank 数据集（\"sentences_allagree\" 配置）\n",
    "fpb = load_dataset('imdb')\n",
    "fpb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e3891-b455-499e-a638-9ab1ff938d49",
   "metadata": {},
   "source": [
    "\n",
    "该数据集包含个训练样本，标签存储在'label'里，文本存储在'sentence'里。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f74e82-4f31-4cc5-bba4-0c4231e84fc6",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02122746-1284-440c-9ee0-cbd6e63942dd",
   "metadata": {},
   "source": [
    "使用BERT WordPiece tokenizer对输入进行符号化（Tokenization）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30f7b421-0c2c-4fb7-acf5-71bd9e27d781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d4386b6f5474f9466316cf4e30cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bc82bffcbb47b4bacf2145502021b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deeec406e17432a95515e60406cee9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples['text'], truncation=True)\n",
    "\n",
    "tokenized_fpb = fpb.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c4f33-505d-4fd6-b092-b00001f9d8ef",
   "metadata": {},
   "source": [
    "### Step 3: Data collator for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd4966-4415-49e7-b9fe-920ccc0e1732",
   "metadata": {},
   "source": [
    "现在，文本已经符号化了，它们需要Hugging Face的Data.to_tf_dataset方法转换为TensorFlow数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c1a1295-e701-47cd-912d-3460eeb0901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbda6d-c732-4131-88fb-51c6d58f243e",
   "metadata": {},
   "source": [
    "### Step 4: Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2a1b700-3c58-4dc6-882b-83c333997757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_fpb['train'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'label'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_data = tokenized_fpb['test'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'label'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7fa73-0c0a-4961-a4da-c42f43e6cf28",
   "metadata": {},
   "source": [
    "### Step 5: Initialize & Compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b445e5-6ed5-4f14-b700-5efed77c04f9",
   "metadata": {},
   "source": [
    "使用 distilbert-base-uncased 模型，该模型已在大规模数据集上预训练。这里使用 TFAutoModelForSequenceClassification 类加载DistilBERT，并指定任务的标签数量（例如情感分析任务的3个类别：正面、负面、中立）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4b717-e995-4372-aded-8eba670288b7",
   "metadata": {},
   "source": [
    "微调过程中，我们没有显式地修改模型架构，因为预训练模型已经为分类任务配置好了输出层。但如果需要添加额外的任务特定层（如线性分类器），可以通过模型的子类化或修改模型结构来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6feeecb-136d-4668-bde4-744219d0d8c2",
   "metadata": {},
   "source": [
    "采用Adam优化器并设置较低的学习率，这是微调预训练模型时常见的做法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caded9ce-5b98-49eb-8e9b-9a93d3fb9fda",
   "metadata": {},
   "source": [
    "对于分类任务，使用了交叉熵损失函数（sparse_categorical_crossentropy），适用于标签是整数格式的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13edbc86-8f1b-4e97-bc57-bd35e985f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # 使用 legacy.Adam 以提升性能\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5fafc-7553-4952-837e-18cd7b3d7cd6",
   "metadata": {},
   "source": [
    "### Step 6: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16ac43-607d-45cc-b327-72279337a91a",
   "metadata": {},
   "source": [
    "现在准备微调。和往常一样在模型上调用fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b391080-3beb-43d2-92c4-8df206525ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 111/1563 [=>............................] - ETA: 2:09:35 - loss: 0.5653 - accuracy: 0.7044"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Reduce learning rate if validation accuracy stops improving\n",
    "# lr_scheduler = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_data,\n",
    "#     validation_data=None,\n",
    "#     epochs=3,\n",
    "#     callbacks=[lr_scheduler]\n",
    "# )\n",
    "\n",
    "hist = model.fit(train_data, validation_data=validation_data, epochs=3)\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# model.save_pretrained('finbert_finetuned')   finbert_finetuned 保存了训练后的模型，你可以进一步在实际任务中使用它进行推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d077b-2923-4772-810a-35999d59e5c1",
   "metadata": {},
   "source": [
    "本次训练是在MacOS系统上进行的训练（Chip：Apple M1 Max，Memory：32GB）上取得时长，供参考。同时在训练的过程中，观察accuracy的变化，来重新调整学习率的数值，并进行重新训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604c1ac-8b68-41de-9887-7cd0bc9bc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# 获取训练和验证的准确率\n",
    "acc = hist.history['accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# 绘制训练准确率曲线\n",
    "plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee57ea-77fb-4630-8f15-c3c22a03a6fd",
   "metadata": {},
   "source": [
    "## FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f4f5c-dd95-455b-9ebf-823788f23c1e",
   "metadata": {},
   "source": [
    "FinBERT采用与Devlin等人（2019）相同的微调架构和优化选择。具体来说，我们使用一个简单的线性层作为分类层，并采用softmax激活函数。损失函数使用交叉熵损失（cross-entropy loss）。通过在金融语料库（PhraseBank、FiQA、AnalystTone）上的预训练，FinBERT显著提升了金融情感分类任务的性能，在多个数据集上相比通用BERT模型均表现出明显的准确率优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00ca7b-4e3e-4be1-902c-a9e581cd0497",
   "metadata": {},
   "source": [
    "\n",
    "### 微调的优势：\n",
    "- **领域适配**：微调能使预训练模型适应特定领域的任务（如FinBERT适用于金融文本）。\n",
    "- **减少数据需求**：相比从零开始训练，微调所需的标注数据更少。\n",
    "- **性能提升**：在微调后，预训练模型通常能在特定任务中实现最先进的性能表现。\n",
    "\n",
    "\n",
    "微调是将预训练模型应用于专业领域和任务的一种高效方法，既利用了模型的通用理解能力，又对其进行了定制化调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30376f-29f6-4ff9-80f3-aff097856410",
   "metadata": {},
   "source": [
    "## 文献\n",
    "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
    "Kristina Toutanova. 2019. Bert: Pre-training of deep\n",
    "bidirectional transformers for language understand-\n",
    "ing. In Proceedings of NAACL, pages 4171–4186.\n",
    "- Yi Yang Mark Christopher Siy UY Allen Huang, FinBERT: A Pretrained Language Model for Financial Communications, http://arxiv.org/abs/2006.08097v2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
