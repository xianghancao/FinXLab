{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2011e3-d88a-4d3d-b94f-d1205b881d92",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37943e3-c4ec-4121-9d39-f8240d6c90ec",
   "metadata": {},
   "source": [
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99188801-5669-4dc2-b0ff-b02c41c1c96e",
   "metadata": {},
   "source": [
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a deep learning model introduced by researchers at Google in 2018. It is a transformer-based model designed for **Natural Language Processing (NLP)** tasks. Hereâ€™s a breakdown of what makes BERT unique and powerful:\n",
    "\n",
    "### 1. **Core Concept**\n",
    "   - **Bidirectionality**: Unlike earlier models like Word2Vec or GloVe, which generate word embeddings based on a single direction (left-to-right or right-to-left context), BERT considers the context from both directions simultaneously. This enables it to better understand the meaning of words in context.\n",
    "\n",
    "### 2. **Transformer Architecture**\n",
    "   - BERT is built on the **Transformer** architecture, specifically focusing on the **encoder** part of it. Transformers rely on mechanisms like **self-attention** to capture relationships between words in a sentence, regardless of their distance.\n",
    "\n",
    "### 3. **Pretraining**\n",
    "   BERT is pre-trained on large corpora using two main tasks:\n",
    "   - **Masked Language Modeling (MLM)**: Some words in a sentence are masked, and BERT learns to predict these masked words based on the context.\n",
    "   - **Next Sentence Prediction (NSP)**: BERT is trained to determine if one sentence logically follows another.\n",
    "\n",
    "### 4. **Applications**\n",
    "   BERT can be fine-tuned for various downstream tasks with minimal additional training, including:\n",
    "   - Text classification (e.g., sentiment analysis)\n",
    "   - Named entity recognition (NER)\n",
    "   - Question answering (e.g., SQuAD dataset)\n",
    "   - Machine translation\n",
    "   - Text summarization\n",
    "\n",
    "### 5. **Advantages**\n",
    "   - **Context-Awareness**: Its bidirectional approach captures nuanced meanings of words.\n",
    "   - **Versatility**: Fine-tuning allows BERT to adapt to specific tasks effectively.\n",
    "   - **Pretrained Models**: Access to pre-trained models like `BERT-base` and `BERT-large` accelerates development.\n",
    "\n",
    "### 6. **Limitations**\n",
    "   - **Resource Intensive**: Training and even fine-tuning BERT requires substantial computational resources.\n",
    "   - **Interpretability**: Like many deep learning models, BERT operates as a \"black box,\" making its decisions harder to interpret.\n",
    "\n",
    "BERT has significantly advanced the field of NLP, inspiring other models like RoBERTa, ALBERT, and T5, which build on its foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bb3f5-2301-4f89-91bd-e2a11573e9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
